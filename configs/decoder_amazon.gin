import data.processed
import modules.model

train.iterations=100000
train.learning_rate=0.0003
train.weight_decay=0.035
train.batch_size=256
train.vae_input_dim=768
train.vae_hidden_dims=[512, 256, 128]
train.vae_embed_dim=32
train.vae_n_cat_feats=0
train.vae_codebook_size=256
train.wandb_logging=False
train.model_type=%modules.model.ModelType.ENCODER_DECODER
# https://api.wandb.ai/links/botta-edoardo-carnegie-mellon-university/9s7a4a9m
train.pretrained_rqvae_path="trained_models/rqvae_amazon_beauty/checkpoint_high_entropy.pt"
train.save_dir_root="out/decoder/amazon/"
train.dataset_folder="dataset/amazon"
train.dataset=%data.processed.RecDataset.AMAZON
train.force_dataset_process=False
train.eval_every=5000
train.dataset_split="beauty"
train.dropout_p=0.2
train.attn_heads=6
train.attn_embed_dim=384
train.attn_layers=12
train.decoder_embed_dim=128